# -*- coding: utf-8 -*-
"""PR_RESERCH PAPER CODE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nlq4gj1Sizs-o_OsKAorNSHbRvpj2oSL
"""

! pip install pgmpy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
data=pd.read_csv("/content/dementia_dataset.csv")
print(data.info())

"""# New Section"""

from pgmpy.models import BayesianModel
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

import pandas as pd
from pgmpy.models import BayesianModel
from pgmpy.estimators import HillClimbSearch, BicScore
from pgmpy.inference import VariableElimination

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values for simplicity (optional)
data = data.dropna()

# Select relevant columns for the Bayesian Network
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Convert categorical variables to numeric codes (if necessary)
data['M/F'] = data['M/F'].astype('category').cat.codes
data['Group'] = data['Group'].astype('category').cat.codes

# Use Hill Climbing Search to learn the structure
hc = HillClimbSearch(data)
best_model = hc.estimate(scoring_method=BicScore(data))

# Print the edges of the learned Bayesian Network
print("Learned structure:", best_model.edges())

# Initialize the Bayesian Model with the learned structure
model = BayesianModel(best_model.edges())

# Fit the model with the data
model.fit(data)

# Perform inference on the model
inference = VariableElimination(model)

# Example: Query the probability distribution of 'Group'
query_result = inference.query(variables=['Group'])
print(query_result)

print(model.get_cpds('EDUC'))

print(model.get_cpds('MMSE'))

import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import HillClimbSearch, BicScore, MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import KBinsDiscretizer

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values for simplicity (optional)
data = data.dropna()

# Select relevant columns for the Bayesian Network
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Convert categorical variables to numeric codes (if necessary)
data['M/F'] = data['M/F'].astype('category').cat.codes
data['Group'] = data['Group'].astype('category').cat.codes

# Discretize continuous variables for better performance with BN
discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')
data[['Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']] = discretizer.fit_transform(data[['Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']])

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.7, random_state=42)

# Use Hill Climb Search to learn the structure from the training data
hc = HillClimbSearch(train_data)
best_model = hc.estimate(scoring_method=BicScore(train_data))

# Initialize the Bayesian Network with the learned structure
model = BayesianNetwork(best_model.edges())
model.fit(train_data, estimator=MaximumLikelihoodEstimator)

# Perform inference using the model
inference = VariableElimination(model)

# Predict the 'Group' variable for the test data
predictions = []
for index, row in test_data.iterrows():
    evidence = {var: row[var] for var in model.nodes if var != 'Group'}
    evidence = {var: int(value) for var, value in evidence.items()}  # Ensure integer values

    try:
        prediction = inference.map_query(variables=['Group'], evidence=evidence)
        predictions.append(prediction['Group'])
    except Exception as e:
        predictions.append(-1)  # Handle prediction failure (e.g., append a default value)

# Calculate accuracy, precision, recall, and F1 score
actual = test_data['Group'].values

accuracy = accuracy_score(actual, predictions)
precision = precision_score(actual, predictions, average='weighted', zero_division=1)
recall = recall_score(actual, predictions, average='weighted', zero_division=1)
f1 = f1_score(actual, predictions, average='weighted', zero_division=1)

# Print results
print("Accuracy of the Bayesian Network model:", accuracy)
print("Precision of the Bayesian Network model:", precision)
print("Recall of the Bayesian Network model:", recall)
print("F1 Score of the Bayesian Network model:", f1)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import tensorflow as tf

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values
data = data.dropna()

# Select relevant columns
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Encode categorical variables
label_encoder = LabelEncoder()
data['M/F'] = label_encoder.fit_transform(data['M/F'])
data['Group'] = label_encoder.fit_transform(data['Group'])

# Split the data into features and target
X = data.drop('Group', axis=1)
y = data['Group']

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Build a feed-forward neural network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')  # Use 'softmax' for multiclass classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Use 'sparse_categorical_crossentropy' for multiclass classification
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)  # Get the class with the highest probability

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy of the Neural Network model:", accuracy)
print("Precision of the Neural Network model:", precision)
print("Recall of the Neural Network model:", recall)
print("F1 Score of the Neural Network model:", f1)

# Ensure 'model' refers to the Bayesian Network instance
bayesian_model = BayesianNetwork(best_model.edges())
bayesian_model.fit(train_data)

# Perform inference using the Bayesian model
inference = VariableElimination(bayesian_model)

# Evaluate accuracy on the training set
train_predictions = []
for index, row in X_train.iterrows():
    evidence = {var: row[var] for var in bayesian_model.nodes if var != 'Group'}
    try:
        prediction = inference.map_query(variables=['Group'], evidence=evidence)
        train_predictions.append(prediction['Group'])
    except IndexError:
        default_prediction = train_data['Group'].mode()[0]
        train_predictions.append(default_prediction)

# Calculate training accuracy
train_accuracy = accuracy_score(y_train, train_predictions)

# Create a DataFrame for plotting accuracy comparison
accuracy_df = pd.DataFrame({
    'Dataset': ['Training', 'Validation'],
    'Accuracy': [train_accuracy, accuracy]
})

# Plotting the accuracy comparison
plt.figure(figsize=(8, 6))
sns.barplot(x='Dataset', y='Accuracy', data=accuracy_df, palette='viridis')
plt.ylim(0, 1)
plt.title("Comparison of Training and Validation Accuracy")
plt.xlabel("Dataset")
plt.ylabel("Accuracy Score")
plt.show()

# Print the accuracies
print("Training Accuracy of the Bayesian Network model:", train_accuracy)
print("Validation Accuracy of the Bayesian Network model:", accuracy)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values
data = data.dropna()

# Select relevant columns
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Encode categorical variables
label_encoder = LabelEncoder()
data['M/F'] = label_encoder.fit_transform(data['M/F'])
data['Group'] = label_encoder.fit_transform(data['Group'])

# Split the data into features and target
X = data.drop('Group', axis=1)
y = data['Group']

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=52)

# Create and train the logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

print("Accuracy of the Logistic Regression model:", accuracy)
print("Precision of the Logistic Regression model:", precision)
print("Recall of the Logistic Regression model:", recall)
print("F1 Score of the Logistic Regression model:", f1)

import pymc as pm
import numpy as np

# Generate some sample data
np.random.seed(0)
data = np.random.normal(0, 1, 100)

# Define a simple non-hierarchical model
with pm.Model() as model:
    mu = pm.Normal("mu", mu=0, sigma=1)
    sigma = pm.HalfNormal("sigma", sigma=1)
    obs = pm.Normal("obs", mu=mu, sigma=sigma, observed=data)

    # Perform inference
    trace = pm.sample(1000, return_inferencedata=True)

# Trace plot
import arviz as az
az.plot_trace(trace)

import pymc as pm
import numpy as np

# Generate some grouped sample data
np.random.seed(1)
group1 = np.random.normal(0, 1, 50)
group2 = np.random.normal(3, 1.5, 50)
data = np.concatenate([group1, group2])
groups = np.array([0]*50 + [1]*50)

# Define a hierarchical model
with pm.Model() as hierarchical_model:
    # Group-level priors
    mu_group = pm.Normal("mu_group", mu=0, sigma=1, shape=2)
    sigma_group = pm.HalfNormal("sigma_group", sigma=1, shape=2)

    # Observation model
    obs = pm.Normal("obs", mu=mu_group[groups], sigma=sigma_group[groups], observed=data)

    # Perform inference
    trace = pm.sample(1000, return_inferencedata=True)

# Trace plot
import arviz as az
az.plot_trace(trace)

import pymc as pm
import numpy as np
import matplotlib.pyplot as plt

# Generate some data
np.random.seed(2)
X = np.linspace(0, 10, 20)
Y = np.sin(X) + np.random.normal(0, 0.1, 20)

# Define the Gaussian Process model
with pm.Model() as gp_model:
    # Mean and covariance functions
    mean_func = pm.gp.mean.Zero()
    cov_func = pm.gp.cov.ExpQuad(1, ls=1)

    # Gaussian Process
    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=cov_func)

    # Likelihood
    sigma = pm.HalfCauchy("sigma", beta=1)
    y_obs = gp.marginal_likelihood("y_obs", X=X[:, None], y=Y, noise=sigma)

    # Perform inference
    trace = pm.sample(1000, return_inferencedata=True)

# Plotting
plt.scatter(X, Y, color="red")
plt.title("Observed Data")
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the output labels
n_classes = len(label_encoder.classes_)
y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))
y_prob = model.predict_proba(X_test)

plt.figure(figsize=(10, 8))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (One-vs-Rest)')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import roc_auc_score

# Compute micro-average ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Micro-average ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Micro-Averaged Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

import numpy as np

# Select two features for 2D plotting
X_train_2d = X_train[:, :2]  # Use the first two features
X_test_2d = X_test[:, :2]

# Train the logistic regression model on these two features
model_2d = LogisticRegression(max_iter=1000)
model_2d.fit(X_train_2d, y_train)

# Create a mesh to plot the decision boundary
h = 0.02
x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1
y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Predict probabilities and reshape for contour plotting
Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plotting
plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test, edgecolor='k', cmap='coolwarm')
plt.title("Logistic Regression Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.colorbar(label='Class')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Use a single feature for visualization
X_single_feature = X_train[:, 0].reshape(-1, 1)  # Choosing the first feature
y_binary = (y_train == 1).astype(int)  # Binary case for simplicity

# Train the logistic regression model
model_single = LogisticRegression(max_iter=1000)
model_single.fit(X_single_feature, y_binary)

# Generate a range of values for the feature
X_range = np.linspace(X_single_feature.min(), X_single_feature.max(), 300).reshape(-1, 1)

# Predict probabilities for this range
y_prob = model_single.predict_proba(X_range)[:, 1]

# Plot the logistic regression curve
plt.figure(figsize=(10, 6))
plt.plot(X_range, y_prob, color='blue', label='Logistic Regression Curve (Sigmoid)')
plt.scatter(X_single_feature, y_binary, color='red', alpha=0.5, label='Data Points')
plt.xlabel('Feature Value')
plt.ylabel('Predicted Probability')
plt.title('Logistic Regression Sigmoid Curve')
plt.legend()
plt.grid()
plt.show()

from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

# Generate learning curve data
train_sizes, train_scores, test_scores = learning_curve(
    LogisticRegression(max_iter=1000),
    X, y,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.plot(train_sizes, test_mean, 'o-', color='red', label='Validation Accuracy')

# Plot the standard deviation as a shaded area
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)

# Labels and title
plt.title('Learning Curve for Logistic Regression')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend(loc='best')
plt.grid()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values for simplicity (optional)
data = data.dropna()

# Select relevant columns for the model
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Encode categorical variables
data['M/F'] = LabelEncoder().fit_transform(data['M/F'])
data['Group'] = LabelEncoder().fit_transform(data['Group'])

# Split the data into features and target
X = data.drop('Group', axis=1)
y = data['Group']

# Standardize the feature variables
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encode the target variable for multi-class classification
y = to_categorical(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Define the DNN model architecture
model = Sequential()
model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(y_train.shape[1], activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_test_actual = np.argmax(y_test, axis=1)

# Calculate accuracy, precision, recall, and F1 score
accuracy = accuracy_score(y_test_actual, y_pred)
precision = precision_score(y_test_actual, y_pred, average='weighted', zero_division=1)
recall = recall_score(y_test_actual, y_pred, average='weighted', zero_division=1)
f1 = f1_score(y_test_actual, y_pred, average='weighted', zero_division=1)

# Print results
print("Accuracy of the DNN model:", accuracy)
print("Precision of the DNN model:", precision)
print("Recall of the DNN model:", recall)
print("F1 Score of the DNN model:", f1)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from tensorflow.keras.utils import to_categorical

# Load the dataset
file_path = '/content/dementia_dataset.csv'
data = pd.read_csv(file_path)

# Drop rows with missing values for simplicity (optional)
data = data.dropna()

# Select relevant columns for the model
columns = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group']
data = data[columns]

# Encode categorical variables
data['M/F'] = LabelEncoder().fit_transform(data['M/F'])
data['Group'] = LabelEncoder().fit_transform(data['Group'])

# Split the data into features and target
X = data.drop('Group', axis=1)
y = data['Group']

# Standardize the feature variables
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape the input data for Conv1D (samples, time steps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# One-hot encode the target variable for multi-class classification
y = to_categorical(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the CNN model architecture
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))
model.add(Conv1D(filters=32, kernel_size=2, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(y_train.shape[1], activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_test_actual = np.argmax(y_test, axis=1)

# Calculate accuracy, precision, recall, and F1 score
accuracy = accuracy_score(y_test_actual, y_pred)
precision = precision_score(y_test_actual, y_pred, average='weighted', zero_division=1)
recall = recall_score(y_test_actual, y_pred, average='weighted', zero_division=1)
f1 = f1_score(y_test_actual, y_pred, average='weighted', zero_division=1)

# Print results
print("Accuracy of the CNN model:", accuracy)
print("Precision of the CNN model:", precision)
print("Recall of the CNN model:", recall)
print("F1 Score of the CNN model:", f1)

import matplotlib.pyplot as plt

# Plot the accuracy over epochs
plt.figure(figsize=(10, 6))

# Plot training accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')

# Plot validation accuracy
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')

# Set the title and labels
plt.title('DNN Model Accuracy vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='best')
plt.grid()
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Concatenate, Dense

# Load or define Model 1
input1 = Input(shape=(64,))
x1 = Dense(32, activation='relu')(input1)
output1 = Dense(16, activation='relu')(x1)
model1 = Model(inputs=input1, outputs=output1)

# Load or define Model 2
input2 = Input(shape=(64,))
x2 = Dense(32, activation='relu')(input2)
output2 = Dense(16, activation='relu')(x2)
model2 = Model(inputs=input2, outputs=output2)

# Combine outputs from both models
combined = Concatenate()([model1.output, model2.output])

# Add a final dense layer for the combined model
x = Dense(32, activation='relu')(combined)
final_output = Dense(1, activation='sigmoid')(x)  # Example for binary classification

# Create the combined model
combined_model = Model(inputs=[model1.input, model2.input], outputs=final_output)

# Compile the combined model
combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary of the combined model
combined_model.summary()

# Example: Train the combined model
# combined_model.fit([data1, data2], labels, epochs=10, batch_size=32)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout

# Define input shape (e.g., time-series data with 100 timesteps and 1 feature)
input_shape = (100, 1)

# Input layer
inputs = Input(shape=input_shape)

# CNN layers
x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
x = MaxPooling1D(pool_size=2)(x)
x = Dropout(0.2)(x)
x = Flatten()(x)

# RNN layers
x = tf.keras.layers.RepeatVector(10)(x)  # Repeat features for RNN processing
x = LSTM(128, return_sequences=True)(x)
x = LSTM(64)(x)

# Fully connected layers
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(1, activation='sigmoid')(x)  # Example: Binary classification

# Create the hybrid model
hybrid_model = Model(inputs=inputs, outputs=outputs)

# Compile the model
hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary of the model
hybrid_model.summary()

# Example: Train the hybrid model
# hybrid_model.fit(data, labels, epochs=20, batch_size=32)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Define the hybrid model with improvements
input_shape = (100, 1)

inputs = Input(shape=input_shape)

# Improved CNN layers
x = Conv1D(filters=128, kernel_size=5, activation='relu')(inputs)
x = BatchNormalization()(x)
x = MaxPooling1D(pool_size=2)(x)
x = Dropout(0.3)(x)
x = Flatten()(x)

# RNN layers with attention mechanism
x = tf.keras.layers.RepeatVector(10)(x)
x = LSTM(256, return_sequences=True)(x)
x = LSTM(128)(x)
x = Dense(128, activation='relu')(x)

# Fully connected layers
x = Dropout(0.5)(x)
outputs = Dense(1, activation='sigmoid')(x)

hybrid_model = Model(inputs=inputs, outputs=outputs)

# Compile with a learning rate scheduler
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
hybrid_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Train the model
history = hybrid_model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    callbacks=[early_stopping, reduce_lr]
)

# Evaluate the model
loss, accuracy = hybrid_model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

# Save the model in the recommended format
hybrid_model.save("hybrid_model.keras")

pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Generate synthetic dataset (replace this with your actual data)
samples = 1000  # Number of samples
timesteps = 100  # Timesteps in each sample
features = 1  # Number of features

# Create random data and imbalanced binary labels
data = np.random.rand(samples, timesteps, features)
labels = np.random.choice([0, 1], size=(samples,), p=[0.9, 0.1])  # 90% class 0, 10% class 1

# Reshape data for scaling
data_reshaped = data.reshape(-1, features)  # (samples * timesteps, features)

# Normalize the data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data_reshaped)
data = data_scaled.reshape(samples, timesteps, features)

# Flatten the data for SMOTE
data_flattened = data.reshape(samples, -1)  # (samples, timesteps * features)

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
data_resampled, labels_resampled = smote.fit_resample(data_flattened, labels)

# Reshape data back to 3D for the hybrid model
data_resampled = data_resampled.reshape(-1, timesteps, features)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_resampled, labels_resampled, test_size=0.2, random_state=42)

# Hybrid model definition (reuse the improved model from above)
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

input_shape = (timesteps, features)

inputs = Input(shape=input_shape)
x = Conv1D(filters=128, kernel_size=5, activation='relu')(inputs)
x = BatchNormalization()(x)
x = MaxPooling1D(pool_size=2)(x)
x = Dropout(0.3)(x)
x = Flatten()(x)
x = tf.keras.layers.RepeatVector(10)(x)
x = LSTM(256, return_sequences=True)(x)
x = LSTM(128)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(1, activation='sigmoid')(x)

hybrid_model = Model(inputs=inputs, outputs=outputs)

# Compile the model
hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Train the model
history = hybrid_model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=30,
    batch_size=32,
    callbacks=[early_stopping, reduce_lr]
)

# Evaluate the model
loss, accuracy = hybrid_model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

# Predict on the test set
y_pred_probs = hybrid_model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Print classification report
report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])
print(report)

from sklearn.metrics import classification_report

# Predict on the test set
y_pred_probs = hybrid_model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Print classification report
report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])
print(report)

import cv2
import numpy as np
from PIL import Image, ImageEnhance

# Load the image
image_path = "/content/Copy of Copy of HybridModel Final.drawio.png"
image = cv2.imread(image_path)

# Increase resolution using interpolation
scaled_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)

# Convert to PIL for additional enhancement
pil_image = Image.fromarray(cv2.cvtColor(scaled_image, cv2.COLOR_BGR2RGB))

# Enhance sharpness
enhancer = ImageEnhance.Sharpness(pil_image)
sharp_image = enhancer.enhance(2.0)

# Enhance contrast
contrast_enhancer = ImageEnhance.Contrast(sharp_image)
enhanced_image = contrast_enhancer.enhance(1.5)

# Save the enhanced image
enhanced_image_path = "/content/Copy of Copy of HybridModel Final.drawio.png"
enhanced_image.save(enhanced_image_path)

print(f"Enhanced image saved at: {enhanced_image_path}")